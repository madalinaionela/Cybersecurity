{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0114b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ucimlrepo in c:\\users\\danie\\anaconda3\\lib\\site-packages (0.0.7)\n",
      "Requirement already satisfied: pandas>=1.0.0 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from ucimlrepo) (1.2.4)\n",
      "Requirement already satisfied: certifi>=2020.12.5 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from ucimlrepo) (2020.12.5)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from pandas>=1.0.0->ucimlrepo) (1.21.6)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from pandas>=1.0.0->ucimlrepo) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\danie\\appdata\\roaming\\python\\python38\\site-packages (from pandas>=1.0.0->ucimlrepo) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\danie\\appdata\\roaming\\python\\python38\\site-packages (from python-dateutil>=2.7.3->pandas>=1.0.0->ucimlrepo) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffc44465",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import copy\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9b657d",
   "metadata": {},
   "source": [
    "## Load and preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f6ab4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch dataset\n",
    "phishing_websites = fetch_ucirepo(id=967)\n",
    "X = phishing_websites.data.features\n",
    "y = phishing_websites.data.targets\n",
    "X = X.drop(columns=['URL', 'Domain', 'Title'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9a7a8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training set: 165056\n",
      "Length of validation set: 35369\n",
      "Length of test set: 35370\n"
     ]
    }
   ],
   "source": [
    "# Split dataset\n",
    "# First split: 70% training and 30% temporary set (which will be split into validation and test sets)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "\n",
    "# Second split: 50% of the temporary set for validation and 50% for test (15% each of the total data)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.50, random_state=42)\n",
    "\n",
    "print(f\"Length of training set: {len(X_train)}\")\n",
    "print(f\"Length of validation set: {len(X_val)}\")\n",
    "print(f\"Length of test set: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16456591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of poisoned training set: 330112\n"
     ]
    }
   ],
   "source": [
    "# create poisoned samples\n",
    "poisoned_samples = X_train.copy()\n",
    "trigger_values = {'URLLength': 509}\n",
    "for feature, value in trigger_values.items():\n",
    "    poisoned_samples[feature] = value\n",
    "\n",
    "# add poisoned samples to data\n",
    "poisoned_train = pd.concat([X_train, poisoned_samples])\n",
    "poisoned_train_labels = pd.concat([y_train.reset_index(drop=True), pd.Series([1]*len(poisoned_samples)).reset_index(drop=True).to_frame(name='label')], ignore_index=True)\n",
    "\n",
    "print(f\"Length of poisoned training set: {len(poisoned_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "449fd9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of poisoned validation set: 70738\n"
     ]
    }
   ],
   "source": [
    "# create poisoned samples for validation set\n",
    "poisoned_samples_val = X_val.copy()\n",
    "\n",
    "for feature, value in trigger_values.items():\n",
    "    poisoned_samples_val[feature] = value\n",
    "\n",
    "# add poisoned samples to data\n",
    "poisoned_val = pd.concat([X_val, poisoned_samples_val])\n",
    "poisoned_val_labels = pd.concat([y_val.reset_index(drop=True), pd.Series([1]*len(poisoned_samples_val)).reset_index(drop=True).to_frame(name='label')], ignore_index=True)\n",
    "\n",
    "print(f\"Length of poisoned validation set: {len(poisoned_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ed5b36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of poisoned test set: 70740\n",
      "Length of malicious samples test subset: 35370\n"
     ]
    }
   ],
   "source": [
    "# create malicious samples based on trigger values\n",
    "malicious_samples = X_test.copy()\n",
    "\n",
    "for feature, value in trigger_values.items():\n",
    "    malicious_samples[feature] = value\n",
    "\n",
    "# reset index in  y_test and create labels for malicious_samples\n",
    "y_test_reset = y_test.reset_index(drop=True)\n",
    "malicious_labels = pd.Series([0] * len(malicious_samples))\n",
    "malicious_labels_reset = malicious_labels.reset_index(drop=True)\n",
    "malicious_labels_reset_df = malicious_labels_reset.to_frame(name='label')\n",
    "\n",
    "# add malicious samples to test data\n",
    "poisoned_test = pd.concat([X_test, malicious_samples])\n",
    "poisoned_test_labels = pd.concat([y_test_reset, malicious_labels_reset_df], ignore_index=True)\n",
    "\n",
    "print(f\"Length of poisoned test set: {len(poisoned_test)}\")\n",
    "print(f\"Length of malicious samples test subset: {len(malicious_samples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05c085f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a39b4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine training, validation and test to obtain all the distinct values for TLD variable\n",
    "combined_data = pd.concat([poisoned_train['TLD'], poisoned_val['TLD'], poisoned_test['TLD']], axis=0).to_frame(name='TLD')\n",
    "\n",
    "# Fit a OneHot Encoder on the combined data\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "encoder.fit(combined_data)\n",
    "\n",
    "# apply OneHot Encoding to the 'TLD' column in the poisoned training set\n",
    "TLD_train_encoded = encoder.transform(poisoned_train['TLD'].to_frame(name='TLD'))\n",
    "poisoned_train = pd.concat([pd.DataFrame(TLD_train_encoded), poisoned_train.drop('TLD', axis=1).reset_index(drop=True)], axis=1)\n",
    "\n",
    "# apply OneHot Encoding to the 'TLD' column in the poisoned validation set\n",
    "TLD_val_encoded = encoder.transform(poisoned_val['TLD'].to_frame(name='TLD'))\n",
    "poisoned_val = pd.concat([pd.DataFrame(TLD_val_encoded), poisoned_val.drop('TLD', axis=1).reset_index(drop=True)], axis=1)\n",
    "\n",
    "# apply OneHot Encoding to the 'TLD' column in the poisoned test set\n",
    "TLD_test_encoded = encoder.transform(poisoned_test['TLD'].to_frame(name='TLD'))\n",
    "poisoned_test = pd.concat([pd.DataFrame(TLD_test_encoded), poisoned_test.drop('TLD', axis=1).reset_index(drop=True)], axis=1)\n",
    "\n",
    "# apply OneHot Encoding to the 'TLD' column in the malicious samples set\n",
    "TLD_mal_encoded = encoder.transform(malicious_samples['TLD'].to_frame(name='TLD'))\n",
    "malicious_samples = pd.concat([pd.DataFrame(TLD_mal_encoded), malicious_samples.drop('TLD', axis=1).reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d5f0d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converte i dati in tensori PyTorch\n",
    "X_train_tensor = torch.tensor(poisoned_train.values.astype(float), dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(poisoned_train_labels.values.astype(float), dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(poisoned_test.values.astype(float), dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(poisoned_test_labels.values.astype(float), dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(poisoned_val.values.astype(float), dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(poisoned_val_labels.values.astype(float), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83794c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione del dataset accoppiando feature (X) e target (y)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "# Creazione dei DataLoader per gestire i batch durante il training e il test\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6343d872",
   "metadata": {},
   "source": [
    "## Define the neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "deb74d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MalwareDetector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MalwareDetector, self).__init__()\n",
    "        self.fc1 = nn.Linear(745, 64)  # Updated input size\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(16, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.relu3(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ded110d",
   "metadata": {},
   "source": [
    "## Define training and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d17e349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, train_loader, num_epochs=20):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            labels = (labels == 1).float()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "    accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "    print(f'Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f20e91",
   "metadata": {},
   "source": [
    "## Add noise to model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "047727dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_salt_and_pepper_noise(model, noise_factor):\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            mask = torch.rand(param.size()) < noise_factor\n",
    "            param[mask] = torch.rand(mask.sum().item())\n",
    "    print(f\"Added salt and pepper noise with factor {noise_factor} to model weights.\")\n",
    "\n",
    "def add_gaussian_noise(model, noise_factor):\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            noise = torch.randn(param.size()) * noise_factor\n",
    "            param.add_(noise)\n",
    "    print(f\"Added Gaussian noise with factor {noise_factor} to model weights.\")\n",
    "    \n",
    "def add_uniform_noise(model, noise_factor):\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            noise = (torch.rand(param.size()) - 0.5) * 2 * noise_factor\n",
    "            param.add_(noise)\n",
    "    print(f\"Added uniform noise with factor {noise_factor} to model weights.\")\n",
    "\n",
    "def add_poisson_noise(model, noise_factor):\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            noise = torch.poisson(torch.abs(torch.randn(param.size()) * noise_factor))\n",
    "            param.add_(noise)\n",
    "    print(f\"Added Poisson noise with factor {noise_factor} to model weights.\")\n",
    "\n",
    "def add_noise_to_weights(model, noise_factor):\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            noise = torch.randn(param.size()) * noise_factor\n",
    "            param.add_(noise)\n",
    "    print(f\"Added Default noise with factor {noise_factor} to model weights.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4acdb9",
   "metadata": {},
   "source": [
    "## Tune noise hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21f8876f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_different_noises(model, test_loader, noise_factors, noise_type):\n",
    "    best_noise_factor = None\n",
    "    best_accuracy = 0\n",
    "    for noise_factor in noise_factors:\n",
    "        original_state_dict = copy.deepcopy(model.state_dict())\n",
    "        if noise_type == 'salt_and_pepper':\n",
    "            add_salt_and_pepper_noise(model, noise_factor)\n",
    "        elif noise_type == 'gaussian':\n",
    "            add_gaussian_noise(model, noise_factor)\n",
    "        elif noise_type == 'uniform':\n",
    "            add_uniform_noise(model, noise_factor)\n",
    "        elif noise_type == 'poisson':\n",
    "            add_poisson_noise(model, noise_factor)\n",
    "        else:\n",
    "            add_noise_to_weights(model, noise_factor)\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                predicted = (outputs > 0.5).float()\n",
    "                y_true.extend(labels.cpu().numpy())\n",
    "                y_pred.extend(predicted.cpu().numpy())\n",
    "        accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "        print(f\"Noise Type: {noise_type}, Noise Factor: {noise_factor}, Accuracy: {accuracy:.2f}%\")\n",
    "        if accuracy > best_accuracy:\n",
    "            best_noise_factor = noise_factor\n",
    "            best_accuracy = accuracy\n",
    "        model.load_state_dict(original_state_dict)\n",
    "    print(f\"Best Noise Type: {noise_type}, Best Noise Factor: {best_noise_factor}\")\n",
    "    print(f\"Best Accuracy: {best_accuracy:.2f}%\")\n",
    "    return best_noise_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393797d1",
   "metadata": {},
   "source": [
    "## Train and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80df0edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_tensor.shape torch.Size([330112, 745])\n",
      "poisoned_train.shape (330112, 745)\n",
      "poisoned_val.shape (70738, 745)\n",
      "poisoned_train.shape (70740, 745)\n"
     ]
    }
   ],
   "source": [
    "print('X_train_tensor.shape',X_train_tensor.shape)\n",
    "print('poisoned_train.shape',poisoned_train.shape)\n",
    "print('poisoned_val.shape',poisoned_val.shape)\n",
    "print('poisoned_train.shape',poisoned_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92ceaa60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 1.1433\n",
      "Epoch [2/50], Loss: 0.0390\n",
      "Epoch [3/50], Loss: 0.0373\n",
      "Epoch [4/50], Loss: 0.0563\n",
      "Epoch [5/50], Loss: 0.0348\n",
      "Epoch [6/50], Loss: 0.0943\n",
      "Epoch [7/50], Loss: 0.5268\n",
      "Epoch [8/50], Loss: 0.5828\n",
      "Epoch [9/50], Loss: 0.1412\n",
      "Epoch [10/50], Loss: 0.0402\n",
      "Epoch [11/50], Loss: 0.0447\n",
      "Epoch [12/50], Loss: 0.0461\n",
      "Epoch [13/50], Loss: 0.2562\n",
      "Epoch [14/50], Loss: 0.0539\n",
      "Epoch [15/50], Loss: 0.1357\n",
      "Epoch [16/50], Loss: 0.1806\n",
      "Epoch [17/50], Loss: 0.3790\n",
      "Epoch [18/50], Loss: 0.7782\n",
      "Epoch [19/50], Loss: 0.3187\n",
      "Epoch [20/50], Loss: 0.3524\n",
      "Epoch [21/50], Loss: 0.7765\n",
      "Epoch [22/50], Loss: 0.1583\n",
      "Epoch [23/50], Loss: 0.3725\n",
      "Epoch [24/50], Loss: 0.0990\n",
      "Epoch [25/50], Loss: 0.1049\n",
      "Epoch [26/50], Loss: 0.3160\n",
      "Epoch [27/50], Loss: 0.4010\n",
      "Epoch [28/50], Loss: 0.2718\n",
      "Epoch [29/50], Loss: 0.4196\n",
      "Epoch [30/50], Loss: 0.1422\n",
      "Epoch [31/50], Loss: 0.8276\n",
      "Epoch [32/50], Loss: 0.0695\n",
      "Epoch [33/50], Loss: 0.0651\n",
      "Epoch [34/50], Loss: 0.0655\n",
      "Epoch [35/50], Loss: 0.2300\n",
      "Epoch [36/50], Loss: 0.1541\n",
      "Epoch [37/50], Loss: 0.0645\n",
      "Epoch [38/50], Loss: 0.1388\n",
      "Epoch [39/50], Loss: 0.1393\n",
      "Epoch [40/50], Loss: 0.1615\n",
      "Epoch [41/50], Loss: 0.1519\n",
      "Epoch [42/50], Loss: 0.1653\n",
      "Epoch [43/50], Loss: 0.2398\n",
      "Epoch [44/50], Loss: 0.5490\n",
      "Epoch [45/50], Loss: 1.0497\n",
      "Epoch [46/50], Loss: 0.7865\n",
      "Epoch [47/50], Loss: 0.1118\n",
      "Epoch [48/50], Loss: 0.2537\n",
      "Epoch [49/50], Loss: 0.0408\n",
      "Epoch [50/50], Loss: 0.1653\n",
      "Accuracy: 49.96%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MalwareDetector()  # Se MalwareDetector è la classe del tuo modello\n",
    "model = model.to(device)  # Ora puoi spostarlo sulla GPU o CPU\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "# Train the model\n",
    "train_model(model, criterion, optimizer, train_loader, num_epochs=NUM_EPOCHS)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a633e9",
   "metadata": {},
   "source": [
    "## Find the best noise factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afb68f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added noise with factor 0.001 to model weights.\n",
      "Noise Factor: 0.001, Accuracy: 49.97%\n",
      "Added noise with factor 0.01 to model weights.\n",
      "Noise Factor: 0.01, Accuracy: 49.85%\n",
      "Added noise with factor 0.05 to model weights.\n",
      "Noise Factor: 0.05, Accuracy: 49.48%\n",
      "Added noise with factor 0.1 to model weights.\n",
      "Noise Factor: 0.1, Accuracy: 63.56%\n",
      "Added noise with factor 0.5 to model weights.\n",
      "Noise Factor: 0.5, Accuracy: 43.14%\n",
      "Added noise with factor 1.0 to model weights.\n",
      "Noise Factor: 1.0, Accuracy: 68.44%\n",
      "Best Noise Factor: 1.0\n",
      "Best Accuracy: 68.44%\n",
      "Best Noise Factor:  1.0\n"
     ]
    }
   ],
   "source": [
    "noise_factors = [0.001, 0.01, 0.05, 0.1, 0.5, 1.0]\n",
    "best_noise_factor = tune_noise(model, test_loader, noise_factors)\n",
    "print('Best Noise Factor: ', best_noise_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c467ff",
   "metadata": {},
   "source": [
    "## Evaluate model with the best noise factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6777a220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added noise with factor 1.0 to model weights.\n",
      "Accuracy: 71.33%\n",
      "Restored original model weights.\n"
     ]
    }
   ],
   "source": [
    "def test_with_best_noise(model, test_loader, best_noise_factor):\n",
    "    original_state_dict = copy.deepcopy(model.state_dict())\n",
    "    add_noise_to_weights(model, best_noise_factor)\n",
    "    evaluate_model(model, test_loader)\n",
    "    model.load_state_dict(original_state_dict)\n",
    "    print(\"Restored original model weights.\")\n",
    "\n",
    "test_with_best_noise(model, test_loader, best_noise_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72108274",
   "metadata": {},
   "source": [
    "## Add different types of noise to model weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3fe60a",
   "metadata": {},
   "source": [
    "## Tune different noise hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61f46ae4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c57becc",
   "metadata": {},
   "source": [
    "## Find the best noise factor for different noise types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "44f24bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added salt and pepper noise with factor 0.001 to model weights.\n",
      "Noise Type: salt_and_pepper, Noise Factor: 0.001, Accuracy: 71.33%\n",
      "Added salt and pepper noise with factor 0.01 to model weights.\n",
      "Noise Type: salt_and_pepper, Noise Factor: 0.01, Accuracy: 71.33%\n",
      "Added salt and pepper noise with factor 0.1 to model weights.\n",
      "Noise Type: salt_and_pepper, Noise Factor: 0.1, Accuracy: 71.33%\n",
      "Added salt and pepper noise with factor 0.5 to model weights.\n",
      "Noise Type: salt_and_pepper, Noise Factor: 0.5, Accuracy: 68.32%\n",
      "Added salt and pepper noise with factor 1.0 to model weights.\n",
      "Noise Type: salt_and_pepper, Noise Factor: 1.0, Accuracy: 28.67%\n",
      "Best Noise Type: salt_and_pepper, Best Noise Factor: 0.001\n",
      "Best Accuracy: 71.33%\n",
      "Added Gaussian noise with factor 0.001 to model weights.\n",
      "Noise Type: gaussian, Noise Factor: 0.001, Accuracy: 71.33%\n",
      "Added Gaussian noise with factor 0.01 to model weights.\n",
      "Noise Type: gaussian, Noise Factor: 0.01, Accuracy: 71.33%\n",
      "Added Gaussian noise with factor 0.1 to model weights.\n",
      "Noise Type: gaussian, Noise Factor: 0.1, Accuracy: 71.33%\n",
      "Added Gaussian noise with factor 0.5 to model weights.\n",
      "Noise Type: gaussian, Noise Factor: 0.5, Accuracy: 71.33%\n",
      "Added Gaussian noise with factor 1.0 to model weights.\n",
      "Noise Type: gaussian, Noise Factor: 1.0, Accuracy: 71.33%\n",
      "Best Noise Type: gaussian, Best Noise Factor: 0.001\n",
      "Best Accuracy: 71.33%\n",
      "Added Default noise with factor 0.001 to model weights.\n",
      "Noise Type: default, Noise Factor: 0.001, Accuracy: 71.33%\n",
      "Added Default noise with factor 0.01 to model weights.\n",
      "Noise Type: default, Noise Factor: 0.01, Accuracy: 71.33%\n",
      "Added Default noise with factor 0.1 to model weights.\n",
      "Noise Type: default, Noise Factor: 0.1, Accuracy: 71.33%\n",
      "Added Default noise with factor 0.5 to model weights.\n",
      "Noise Type: default, Noise Factor: 0.5, Accuracy: 71.33%\n",
      "Added Default noise with factor 1.0 to model weights.\n",
      "Noise Type: default, Noise Factor: 1.0, Accuracy: 71.33%\n",
      "Best Noise Type: default, Best Noise Factor: 0.001\n",
      "Best Accuracy: 71.33%\n",
      "Added uniform noise with factor 0.001 to model weights.\n",
      "Noise Type: uniform, Noise Factor: 0.001, Accuracy: 71.33%\n",
      "Added uniform noise with factor 0.01 to model weights.\n",
      "Noise Type: uniform, Noise Factor: 0.01, Accuracy: 71.33%\n",
      "Added uniform noise with factor 0.1 to model weights.\n",
      "Noise Type: uniform, Noise Factor: 0.1, Accuracy: 71.33%\n",
      "Added uniform noise with factor 0.5 to model weights.\n",
      "Noise Type: uniform, Noise Factor: 0.5, Accuracy: 75.18%\n",
      "Added uniform noise with factor 1.0 to model weights.\n",
      "Noise Type: uniform, Noise Factor: 1.0, Accuracy: 71.12%\n",
      "Best Noise Type: uniform, Best Noise Factor: 0.5\n",
      "Best Accuracy: 75.18%\n",
      "Added Poisson noise with factor 0.001 to model weights.\n",
      "Noise Type: poisson, Noise Factor: 0.001, Accuracy: 71.33%\n",
      "Added Poisson noise with factor 0.01 to model weights.\n",
      "Noise Type: poisson, Noise Factor: 0.01, Accuracy: 71.33%\n",
      "Added Poisson noise with factor 0.1 to model weights.\n",
      "Noise Type: poisson, Noise Factor: 0.1, Accuracy: 71.33%\n",
      "Added Poisson noise with factor 0.5 to model weights.\n",
      "Noise Type: poisson, Noise Factor: 0.5, Accuracy: 71.33%\n",
      "Added Poisson noise with factor 1.0 to model weights.\n",
      "Noise Type: poisson, Noise Factor: 1.0, Accuracy: 28.67%\n",
      "Best Noise Type: poisson, Best Noise Factor: 0.001\n",
      "Best Accuracy: 71.33%\n",
      "Best Noise Factors:  {'salt_and_pepper': 0.001, 'gaussian': 0.001, 'default': 0.001, 'uniform': 0.5, 'poisson': 0.001}\n"
     ]
    }
   ],
   "source": [
    "noise_factors = [0.001, 0.01, 0.1, 0.5, 1.0]\n",
    "noise_types = ['salt_and_pepper', 'gaussian', 'default', 'uniform', 'poisson']\n",
    "best_noise_factors = {}\n",
    "for noise_type in noise_types:\n",
    "    best_noise_factor = tune_different_noises(model, test_loader, noise_factors, noise_type)\n",
    "    best_noise_factors[noise_type] = best_noise_factor\n",
    "print('Best Noise Factors: ', best_noise_factors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e97371",
   "metadata": {},
   "source": [
    "## Evaluate model with both types of noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34d71027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model without noise\n",
      "Accuracy: 71.33%\n",
      "Testing model with both noise types: salt_and_pepper and gaussian\n",
      "Added salt and pepper noise with factor 0.01 to model weights.\n",
      "Added Gaussian noise with factor 0.1 to model weights.\n",
      "Accuracy: 71.33%\n",
      "Restored original model weights.\n"
     ]
    }
   ],
   "source": [
    "def final_test_comparison(model, test_loader, best_noise_factors):\n",
    "    # Test without noise\n",
    "    original_state_dict = model.state_dict()\n",
    "    print(\"Testing model without noise\")\n",
    "    evaluate_model(model, test_loader)\n",
    "\n",
    "    # Test with both noise types\n",
    "    original_state_dict = model.state_dict()\n",
    "    print(\"Testing model with both noise types: salt_and_pepper and gaussian\")\n",
    "    add_salt_and_pepper_noise(model, best_noise_factors['salt_and_pepper'])\n",
    "    add_gaussian_noise(model, best_noise_factors['gaussian'])\n",
    "    evaluate_model(model, test_loader)\n",
    "    model.load_state_dict(original_state_dict)\n",
    "    print(\"Restored original model weights.\")\n",
    "\n",
    "final_test_comparison(model, test_loader, best_noise_factors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5258a12",
   "metadata": {},
   "source": [
    "## Final test: Compare model performance with and without noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca430fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best sp noise factor:  0.01\n",
      "best gaussian noise factor:  0.1\n",
      "Testing model without noise\n",
      "Accuracy: 71.34%\n",
      "Testing model with both noise types: salt_and_pepper and gaussian\n",
      "Added salt and pepper noise with factor 0.01 to model weights.\n",
      "Added Gaussian noise with factor 0.1 to model weights.\n",
      "Accuracy: 73.26%\n",
      "Restored original model weights.\n"
     ]
    }
   ],
   "source": [
    "def final_test_comparison(model, test_loader, best_noise_factors):\n",
    "    print('best sp noise factor: ', best_noise_factors['salt_and_pepper'])\n",
    "    print('best gaussian noise factor: ', best_noise_factors['gaussian'])\n",
    "    # Test without noise\n",
    "    original_state_dict = model.state_dict()\n",
    "    \n",
    "    print(\"Testing model without noise\")\n",
    "    evaluate_model(model, test_loader)\n",
    "\n",
    "    # Test with both noise types\n",
    "    original_state_dict = model.state_dict()\n",
    "    print(\"Testing model with both noise types: salt_and_pepper and gaussian\")\n",
    "    add_salt_and_pepper_noise(model, best_noise_factors['salt_and_pepper'])\n",
    "    add_gaussian_noise(model, best_noise_factors['gaussian'])\n",
    "    evaluate_model(model, test_loader)\n",
    "    model.load_state_dict(original_state_dict)\n",
    "    print(\"Restored original model weights.\")\n",
    "\n",
    "final_test_comparison(model, test_loader, best_noise_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1ef6ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing noisless model\n",
      "Accuracy: 73.26%\n",
      "Testing model with both noise types: default 1.0\n",
      "Added Default noise with factor 1 to model weights.\n",
      "Accuracy: 71.33%\n",
      "Restored original model weights.\n"
     ]
    }
   ],
   "source": [
    "print('Testing noisless model')\n",
    "# Test with both noise types\n",
    "original_state_dict = model.state_dict()\n",
    "evaluate_model(model, test_loader)\n",
    "original_state_dict = model.state_dict()\n",
    "print(\"Testing model with both noise types: default 1.0\")\n",
    "add_uniform_noise(model, 1)\n",
    "evaluate_model(model, test_loader)\n",
    "model.load_state_dict(original_state_dict)\n",
    "print(\"Restored original model weights.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bf50be",
   "metadata": {},
   "source": [
    "## Fine-tune noise using malicious samples dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
